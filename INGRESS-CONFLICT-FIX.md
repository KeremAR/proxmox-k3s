# Ingress Conflict Fix - Remove Nginx Traffic Routing from Rollouts

## üî¥ Problem

After migrating to service-based pattern, ArgoCD couldn't create `staging-frontend-ingress` because:

```
admission webhook "validate.nginx.ingress.kubernetes.io" denied the request: 
host "todo-app-staging.192.168.0.111.nip.io" and path "/" is already defined in ingress staging/staging-todo-app
```

**Existing Ingresses in staging namespace:**
```bash
kubectl get ingress -n staging
NAME                                   CLASS   HOSTS                                   AGE
staging-todo-app                       nginx   todo-app-staging.192.168.0.111.nip.io   17d  # ‚ùå Old umbrella chart
user-service-staging-todo-app-canary   nginx   todo-app-staging.192.168.0.111.nip.io   12d  # Argo Rollouts
todo-service-staging-todo-app-canary   nginx   todo-app-staging.192.168.0.111.nip.io   12d  # Argo Rollouts
frontend-staging-todo-app-canary       nginx   todo-app-staging.192.168.0.111.nip.io   12d  # Argo Rollouts
```

**Root Cause:**
- Old `staging-todo-app` Ingress (from umbrella chart) still exists
- Argo Rollouts was configured with `trafficRouting.nginx.stableIngress: staging-todo-app`
- This created 3 canary Ingresses (one per service)
- New `staging-frontend-ingress` conflicts with same host/path

---

## ‚úÖ Solution: Remove Nginx Traffic Routing

**Why this is correct:**
1. **Frontend has Caddy reverse proxy** ‚Üí Routes /users* to user-service:8001, /todos* to todo-service:8002
2. **Ingress only routes to frontend** ‚Üí No need for Ingress-level canary routing
3. **Canary deployments still work** ‚Üí Service-level canary (user-service + user-service-canary) handles it
4. **AnalysisTemplates still validate** ‚Üí Health checks run on canary services before promotion

---

## üîß Changes Made

### 1. **Removed `trafficRouting.nginx` from all Rollout templates:**

**user-service/templates/deployment.yaml:**
```yaml
# ‚ùå REMOVED:
      trafficRouting:
        nginx:
          stableIngress: {{ .Values.canary.stableIngress }}
```

**todo-service/templates/deployment.yaml:**
```yaml
# ‚ùå REMOVED:
      trafficRouting:
        nginx:
          stableIngress: {{ .Values.canary.stableIngress }}
```

**frontend/templates/deployment.yaml:**
```yaml
# ‚ùå REMOVED:
      trafficRouting:
        nginx:
          stableIngress: {{ .Values.canary.stableIngress }}
```

### 2. **Removed `stableIngress` from values.yaml:**

**user-service/values.yaml:**
```yaml
canary:
  enabled: true
  steps: [...]
  analysisTemplateName: check-pod-readiness
  # stableIngress: staging-todo-app  # ‚ùå REMOVED
```

**todo-service/values.yaml:**
```yaml
canary:
  enabled: true
  steps: [...]
  analysisTemplateName: check-pod-readiness
  # stableIngress: staging-todo-app  # ‚ùå REMOVED
```

**frontend/values.yaml:**
```yaml
canary:
  enabled: true
  steps: [...]
  analysisTemplateName: check-frontend-readiness
  # stableIngress: staging-todo-app  # ‚ùå REMOVED
```

---

## üöÄ Deployment Steps

### 1. **Delete Old Ingresses:**
```bash
# Delete old umbrella chart Ingress
kubectl delete ingress staging-todo-app -n staging

# Delete old canary Ingresses (Argo Rollouts will recreate if needed, but won't because we removed trafficRouting)
kubectl delete ingress user-service-staging-todo-app-canary -n staging
kubectl delete ingress todo-service-staging-todo-app-canary -n staging
kubectl delete ingress frontend-staging-todo-app-canary -n staging
```

### 2. **Commit and Push Changes:**
```powershell
cd C:\Users\kerem\Documents\proxmox-k3s

git add helm-charts/
git commit -m "fix: Remove Nginx traffic routing from Rollouts

- Remove trafficRouting.nginx from all Rollout templates
- Remove stableIngress configuration from values.yaml
- Ingress-level canary routing not needed (Caddy handles backend routing)
- Service-level canary still works (stable + canary services)
- Fixes Ingress conflict: staging-frontend-ingress can now be created"

git push origin main
```

### 3. **Sync ArgoCD:**
```bash
# Sync all services (this will update Rollouts and create new Ingress)
argocd app sync staging-user-service
argocd app sync staging-todo-service
argocd app sync staging-frontend

# Wait for sync
argocd app wait staging-frontend --health
```

### 4. **Verify New Ingress:**
```bash
kubectl get ingress -n staging
# Expected: Only staging-frontend-ingress

kubectl describe ingress staging-frontend-ingress -n staging
# Should show: todo-app-staging.192.168.0.111.nip.io ‚Üí frontend:3000
```

### 5. **Test Application:**
```bash
curl http://todo-app-staging.192.168.0.111.nip.io
# Should return frontend HTML

curl http://todo-app-staging.192.168.0.111.nip.io/users/health
# Should return user-service health (routed by Caddy)

curl http://todo-app-staging.192.168.0.111.nip.io/todos/health
# Should return todo-service health (routed by Caddy)
```

---

## üìä Traffic Flow (After Fix)

### **Old Pattern (With Nginx Traffic Routing):**
```
Internet ‚Üí Nginx Ingress (staging-todo-app) ‚Üí [Canary Ingress Logic] ‚Üí user-service/todo-service/frontend
                                                     ‚Üì
                                        Creates 3 canary Ingresses
```

### **New Pattern (Without Nginx Traffic Routing):**
```
Internet ‚Üí Nginx Ingress (staging-frontend-ingress) ‚Üí frontend:3000 (Caddy)
                                                           ‚Üì
                                          Caddy reverse proxy routes:
                                          - /users* ‚Üí user-service:8001
                                          - /todos* ‚Üí todo-service:8002
                                          - /* ‚Üí Static files
```

**Canary Deployment Still Works:**
```
Rollout creates:
  - user-service (stable)     ‚Üê 80% traffic
  - user-service-canary       ‚Üê 20% traffic

Kubernetes Service-level routing handles canary split!
```

---

## ‚úÖ Benefits

1. **No Ingress conflicts** ‚Üí Only 1 Ingress (`staging-frontend-ingress`)
2. **Simpler architecture** ‚Üí Caddy handles all routing
3. **Canary still works** ‚Üí Service-level canary (not Ingress-level)
4. **Less K8s resources** ‚Üí No canary Ingresses per service
5. **Easier to debug** ‚Üí Single entry point, clear traffic flow

---

## üéØ Verification Checklist

- [ ] Old `staging-todo-app` Ingress deleted
- [ ] Old canary Ingresses deleted (user-service-*, todo-service-*, frontend-*)
- [ ] New `staging-frontend-ingress` created successfully
- [ ] Ingress routes to `frontend:3000`
- [ ] Frontend Caddy routes to backend services correctly
- [ ] Canary deployments still trigger (setWeight: 20, 40, 60)
- [ ] AnalysisTemplates still run health checks
- [ ] Application accessible via `http://todo-app-staging.192.168.0.111.nip.io`

---

## üìù Notes

**Why we don't need Ingress-level canary routing:**

1. **Backend services are not exposed via Ingress** ‚Üí They're ClusterIP services
2. **Frontend is the only public entry point** ‚Üí Caddy proxies to backends
3. **Service-level canary is sufficient** ‚Üí Kubernetes handles traffic split between stable and canary services
4. **AnalysisTemplates validate health** ‚Üí Health checks ensure canary is ready before promotion

**If we wanted Ingress-level canary (not recommended for this architecture):**
- Would need to expose user-service and todo-service via Ingress paths
- Would need `stableIngress: staging-frontend-ingress` in all Rollouts
- Would create 3 canary Ingresses again (defeating the purpose of single Ingress)
