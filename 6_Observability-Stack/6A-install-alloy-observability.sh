#!/bin/bash

echo "=== Installing Grafana Alloy Observability Stack ==="
echo ""

# Step 0: Install Helm if not already installed
echo "Step 0: Checking Helm installation..."
if ! command -v helm &> /dev/null; then
    echo "Helm not found. Installing Helm..."
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
    echo "‚úÖ Helm installed successfully"
else
    HELM_VERSION=$(helm version --short)
    echo "‚úÖ Helm already installed: $HELM_VERSION"
fi
echo ""

# Step 1: Create observability namespace
echo "Step 1: Creating observability namespace..."
kubectl create namespace observability --dry-run=client -o yaml | kubectl apply -f -

helm repo add grafana https://grafana.github.io/helm-charts
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update



# === Adƒ±m 1: Loki Veritabanƒ± Kurulumu ===
echo ""
echo "Step 1: Installing Loki Database..."

cat <<EOF > /tmp/loki-values.yaml
# --- D√úZELTME (T√ºm Loki Hatalarƒ± ƒ∞√ßin DOƒûRU YAPI) ---

# 1. K√∂k dizin ayarlarƒ±
chunksCache:
  enabled: false
resultsCache:
  enabled: false
minio:
  enabled: false
deploymentMode: SingleBinary
singleBinary:
  replicas: 1
write:
  replicas: 0
read:
  replicas: 0
backend:
  replicas: 0
test:
  enabled: false

# 2. T√ºm loki konfig√ºrasyonu 'loki:' anahtarƒ± ALTINDA
loki:
  auth_enabled: false
  commonConfig:
    replication_factor: 1
  
  # 3. 'storage' bloƒüu 'loki:' altƒ±nda
  #    VE DOƒûRU anahtar 'chunks_directory'
  storage:
    type: filesystem
    filesystem:
      chunks_directory: /var/loki/chunks
      rules_directory: /var/loki/rules
      
  # 4. 'useTestSchema: false' (varsayƒ±lan) olduƒüu i√ßin,
  #    'filesystem' kullanan √∂zel ≈üema tanƒ±mƒ±.
  #    'bucketNames' hatasƒ±nƒ± bu √ß√∂zer.
  schemaConfig:
    configs:
      - from: 2024-01-01
        store: tsdb
        object_store: filesystem 
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
# --- D√úZELTME Bƒ∞Tƒ∞≈ûƒ∞ ---
EOF

helm upgrade --install loki grafana/loki \
  --namespace observability \
  --create-namespace \
  --values /tmp/loki-values.yaml \
  --wait

echo "‚úÖ Loki Database installed!"

# === Adƒ±m 2: Prometheus Veritabanƒ± Kurulumu ===
echo ""
echo "Step 2: Installing Prometheus Database..."

cat <<EOF > /tmp/prometheus-values.yaml
server:
  nodeSelector:
    kubernetes.io/hostname: k3s-worker

  persistentVolume:
    enabled: true
    storageClass: local-path
    accessModes: ["ReadWriteOnce"]
    size: 10Gi

  # Enable remote write receiver - CRITICAL for Alloy!
  extraArgs:
    web.enable-remote-write-receiver: ""

# --- D√úZELTME (Prometheus √á√∂kme Hatasƒ± i√ßin) ---
# Chart'ƒ±n varsayƒ±lan (default) values.yaml'daki uzun
# 'scrape_configs' listesini eziyoruz (override).
# Sadece 'prometheus' (kendisi) i≈üini bƒ±rakƒ±yoruz.
# Bu, √ß√∂kmesini engelleyecektir.
serverFiles:
  prometheus.yml:
    rule_files:
      - /etc/config/recording_rules.yml
      - /etc/config/alerting_rules.yml
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

alertmanager:
  enabled: false
prometheus-pushgateway:
  enabled: false
prometheus-node-exporter:
  enabled: false
kube-state-metrics:
  enabled: false
EOF

helm upgrade --install prometheus prometheus-community/prometheus \
  --namespace observability \
  --values /tmp/prometheus-values.yaml \
  --wait

echo "‚úÖ Prometheus Database installed!"

# === Adƒ±m 3: Grafana Aray√ºz√º Kurulumu ===
echo ""
echo "Step 3: Installing Grafana UI..."

cat <<EOF > /tmp/grafana-values.yaml
adminPassword: admin123
service:
  type: ClusterIP
nodeSelector:
  kubernetes.io/hostname: k3s-worker
securityContext:
  fsGroup: 472
  runAsGroup: 472
  runAsUser: 472

# Enable sidecar for automatic dashboard loading from ConfigMaps
sidecar:
  dashboards:
    enabled: true
    label: grafana_dashboard
    labelValue: "1"
    folder: /tmp/dashboards
    folderAnnotation: grafana_folder
    provider:
      foldersFromFilesStructure: true

datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.observability.svc.cluster.local:80
      access: proxy
      isDefault: true

    # --- D√úZELTME BURADA ---
    - name: Loki
      type: loki
      # Port 3100 DEƒûƒ∞L, Port 80
      url: http://loki-gateway.observability.svc.cluster.local:80
      access: proxy
    # --- D√úZELTME Bƒ∞TTƒ∞ ---
EOF

helm upgrade --install grafana grafana/grafana \
  --namespace observability \
  --values /tmp/grafana-values.yaml \
  --wait

echo "‚úÖ Grafana UI installed!"


# === Adƒ±m 4: Grafana Alloy Ajanƒ± Kurulumu ===

cat <<'EOF' > /tmp/alloy-full-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-full-config
  namespace: observability
data:
  config.alloy: |
    // ========================================
    // === LOG COLLECTION (LOKI) ===
    // ========================================

    discovery.kubernetes "pods" {
      role = "pod"
    }

    discovery.relabel "pod_logs" {
      targets = discovery.kubernetes.pods.targets
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
        separator     = "/"
        target_label  = "job"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        separator     = "/"
        action        = "replace"
        replacement   = "/var/log/pods/*$1/*.log"
        target_label  = "__path__"
      }
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_pod_container_id"]
        regex = "^(\\w+)://.+$"
        replacement = "$1"
        target_label = "tmp_container_runtime"
      }
    }

    local.file_match "pod_logs" {
      path_targets = discovery.relabel.pod_logs.output
    }

    loki.source.file "pod_logs" {
      targets    = local.file_match.pod_logs.targets
      forward_to = [loki.process.pod_logs.receiver]
    }

    loki.process "pod_logs" {
      stage.match {
        selector = "{tmp_container_runtime=\"containerd\"}"
        stage.cri {}
      }
      stage.match {
        selector = "{tmp_container_runtime=\"docker\"}"
        stage.docker {}
      }
      stage.label_drop {
        values = ["tmp_container_runtime"]
      }

      forward_to = [loki.write.loki_db.receiver]
    }

    loki.write "loki_db" {
      endpoint {
        url = "http://loki-gateway.observability.svc.cluster.local:80/loki/api/v1/push"
      }
    }

    // ========================================
    // === METRICS COLLECTION (PROMETHEUS) ===
    // ========================================

    // --- 1. NODE-LEVEL METRICS: Unix Exporter (node-exporter equivalent) ---
    prometheus.exporter.unix "unix" {
      procfs_path = "/host/proc"
      sysfs_path  = "/host/sys"
      rootfs_path = "/host/root"
    }

    prometheus.scrape "unix" {
      targets    = prometheus.exporter.unix.unix.targets
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }

    // --- 2. NODE-LEVEL METRICS: Kubelet (includes cAdvisor metrics) ---
    // K3s kubelet exposes container metrics at /metrics/cadvisor endpoint
    discovery.kubernetes "k8s_nodes" {
      role = "node"
    }

    discovery.relabel "kubelet" {
      targets = discovery.kubernetes.k8s_nodes.targets
      
      // Set address to node IP and port
      rule {
        source_labels = ["__meta_kubernetes_node_address_InternalIP"]
        target_label  = "__address__"
        replacement   = "$1:10250"
      }
      
      // Set metrics path to cAdvisor endpoint
      rule {
        replacement  = "/metrics/cadvisor"
        target_label = "__metrics_path__"
      }
      
      // Set scheme to HTTPS
      rule {
        replacement  = "https"
        target_label = "__scheme__"
      }
      
      // Add node label
      rule {
        source_labels = ["__meta_kubernetes_node_name"]
        target_label  = "node"
      }
      
      // Add job label
      rule {
        replacement  = "kubelet"
        target_label = "job"
      }
    }

    prometheus.scrape "kubelet" {
      targets    = discovery.relabel.kubelet.output
      
      // Kubelet uses self-signed certs, skip verification
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      tls_config {
        insecure_skip_verify = true
      }
      
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }

    // --- 3. CLUSTER-LEVEL METRICS: Pod Discovery ---
    discovery.kubernetes "k8s_pods" {
      role = "pod"
    }

    discovery.relabel "k8s_pods" {
      targets = discovery.kubernetes.k8s_pods.targets
      
      // Keep only pods with prometheus.io/scrape=true annotation
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_scrape"]
        action        = "keep"
        regex         = "true"
      }
      
      // Use custom port from annotation if specified
      rule {
        source_labels = ["__address__", "__meta_kubernetes_pod_annotation_prometheus_io_port"]
        action        = "replace"
        target_label  = "__address__"
        regex         = "([^:]+)(?::\\d+)?;(\\d+)"
        replacement   = "$1:$2"
      }
      
      // Use custom metrics path from annotation if specified (default /metrics)
      rule {
        source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_path"]
        action        = "replace"
        target_label  = "__metrics_path__"
        regex         = "(.+)"
      }
      
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label  = "pod"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label  = "container"
      }
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        target_label  = "pod_phase"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
        separator     = "/"
        target_label  = "job"
      }
    }

    prometheus.scrape "k8s_pods" {
      targets    = discovery.relabel.k8s_pods.output
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }

    // --- 4. ARGO ROLLOUTS METRICS ---
    prometheus.scrape "argo_rollouts" {
      targets = [{
        __address__ = "argo-rollouts-metrics.argo-rollouts.svc.cluster.local:8090",
        job         = "argo-rollouts",
      }]
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }

    // --- 5. CLUSTER-LEVEL METRICS: Service Discovery (DISABLED) ---
    // Service discovery disabled - Pod discovery is sufficient for application metrics
    // To enable: uncomment the section below and add prometheus.io/scrape annotation to Service manifests
    /*
    discovery.kubernetes "k8s_services" {
      role = "service"
    }

    discovery.relabel "k8s_services" {
      targets = discovery.kubernetes.k8s_services.targets
      
      // Keep only services with prometheus.io/scrape=true annotation
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_scrape"]
        action        = "keep"
        regex         = "true"
      }
      
      // Use custom port from annotation if specified
      rule {
        source_labels = ["__address__", "__meta_kubernetes_service_annotation_prometheus_io_port"]
        action        = "replace"
        target_label  = "__address__"
        regex         = "([^:]+)(?::\\d+)?;(\\d+)"
        replacement   = "$1:$2"
      }
      
      // Use custom metrics path from annotation if specified
      rule {
        source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_path"]
        action        = "replace"
        target_label  = "__metrics_path__"
        regex         = "(.+)"
      }
      
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label  = "namespace"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_name"]
        target_label  = "service"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_type"]
        target_label  = "type"
      }
      rule {
        source_labels = ["__meta_kubernetes_service_port_number"]
        target_label  = "port_number"
      }
      rule {
        source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_service_name"]
        separator     = "/"
        target_label  = "job"
      }
    }

    prometheus.scrape "k8s_services" {
      targets    = discovery.relabel.k8s_services.output
      forward_to = [prometheus.remote_write.prometheus.receiver]
    }
    */

    // --- 5. REMOTE WRITE: Send all metrics to Prometheus ---
    prometheus.remote_write "prometheus" {
      endpoint {
        url = "http://prometheus-server.observability.svc.cluster.local:80/api/v1/write"
      }
    }

    // ==========================================
    // OPENTELEMETRY TRACING CONFIGURATION
    // ==========================================

    // --- 6. OTEL RECEIVER: Accept traces from Python services ---
    otelcol.receiver.otlp "default" {
      // gRPC endpoint
      grpc {
        endpoint = "0.0.0.0:4317"
      }

      // HTTP endpoint
      http {
        endpoint = "0.0.0.0:4318"
      }

      output {
        traces  = [otelcol.exporter.otlp.jaeger.input]
      }
    }

    // --- 7. OTEL EXPORTER: Forward traces to Jaeger ---
    otelcol.exporter.otlp "jaeger" {
      client {
        endpoint = "jaeger-collector.observability.svc.cluster.local:4317"
        tls {
          insecure = true
        }
      }
    }
EOF
kubectl apply -f /tmp/alloy-full-config.yaml

echo ""
echo "Step 4: Installing Grafana Alloy (The *ONE* Agent)..."

cat <<EOF > /tmp/alloy-values.yaml
controller:
  type: 'daemonset'
  
  volumes:
    extra:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /

alloy:
  configMap:
    create: false
    name: alloy-full-config
    key: config.alloy

  mounts:
    varlog: true
    dockercontainers: true
    extra:
      - name: proc
        mountPath: /host/proc
        readOnly: true
      - name: sys
        mountPath: /host/sys
        readOnly: true
      - name: root
        mountPath: /host/root
        readOnly: true
        mountPropagation: HostToContainer

# Expose OTLP ports for trace ingestion
service:
  type: ClusterIP
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP

rbac:
  create: true
EOF

helm upgrade --install alloy grafana/alloy \
  --namespace observability \
  --values /tmp/alloy-values.yaml \
  --wait

echo "‚úÖ Grafana Alloy Agent installed!"

# Step 4: Install kube-state-metrics for Pod Metrics
echo ""
echo "Step 4: Installing kube-state-metrics..."
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

cat <<EOF > /tmp/kube-state-metrics-values.yaml
prometheus:
  monitor:
    enabled: false

selfMonitor:
  enabled: false

prometheusScrape: true

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"
EOF

helm upgrade --install kube-state-metrics prometheus-community/kube-state-metrics \
  --namespace observability \
  --values /tmp/kube-state-metrics-values.yaml \
  --wait
echo "‚úÖ kube-state-metrics installed!"

# Step 5: Create Ingress Routes
echo ""
echo "Step 5: Creating Ingress routes..."

# Get Nginx Ingress LoadBalancer IP
INGRESS_IP=$(kubectl get service nginx-ingress-loadbalancer -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

if [ -z "$INGRESS_IP" ]; then
    echo "‚ö†Ô∏è  Warning: Nginx Ingress LoadBalancer not found"
    echo "   Skipping Ingress creation. Services accessible via ClusterIP only."
else
    echo "‚úÖ Found LoadBalancer IP: $INGRESS_IP"
    
    # Create Grafana Ingress
    echo "üìù Creating Grafana Ingress..."
    cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana-ingress
  namespace: observability
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: grafana.${INGRESS_IP}.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 80
EOF

    # Create Prometheus Ingress
    echo "üìù Creating Prometheus Ingress..."
    cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prometheus-ingress
  namespace: observability
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: prometheus.${INGRESS_IP}.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus-server
            port:
              number: 9090
EOF

cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: alloy-ingress
  namespace: observability
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: alloy.${INGRESS_IP}.nip.io
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: alloy
            port:
              number: 12345
EOF
    
    echo "‚úÖ Ingress routes created!"
fi

# Step 6: Verification
echo ""
echo "=== Verification ==="
echo ""

echo "Pods in observability namespace:"
kubectl get pods -n observability
echo ""

echo "Services in observability namespace:"
kubectl get svc -n observability
echo ""

echo "=== Installation Complete ==="
echo ""
if [ -z "$INGRESS_IP" ]; then
    echo "‚ö†Ô∏è  Services accessible via ClusterIP (Nginx Ingress not found)"
    echo ""
    echo "Internal Access:"
    echo "  - Grafana:    prometheus-grafana.observability.svc.cluster.local:80"
    echo "  - Prometheus: prometheus-kube-prometheus-prometheus.observability.svc.cluster.local:9090"
else
    echo "üîó Access URLs:"
    echo "  - Grafana:    http://grafana.${INGRESS_IP}.nip.io (admin / admin123)"
    echo "  - Prometheus: http://prometheus.${INGRESS_IP}.nip.io"
    echo "  - Alloy UI:   http://alloy.${INGRESS_IP}.nip.io"
    echo ""
    echo "‚ö†Ô∏è  Note: nip.io automatically resolves <name>.<IP>.nip.io ‚Üí <IP>"
    echo "   No /etc/hosts editing needed!"
fi
echo ""
echo "Next Steps:"
echo "1. Access Grafana and go to Explore"
echo "2. For LOGS - Select 'Loki' datasource and query: {namespace=\"kube-system\"}"
echo "3. For METRICS - Select 'Prometheus' datasource and query examples:"
echo "   - Node CPU: node_cpu_seconds_total"
echo "   - Container Memory: container_memory_usage_bytes"
echo "   - Pod Metrics: kube_pod_info"
echo ""
echo "Components:"
echo "  - Loki: Log storage"
echo "  - Prometheus: Metric storage"
echo "  - Grafana Alloy: Unified collector (DaemonSet)"
echo "    ‚îú‚îÄ Logs: Pod logs ‚Üí Loki"
echo "    ‚îî‚îÄ Metrics:"
echo "       ‚îú‚îÄ Kubelet /metrics/cadvisor (container metrics)"
echo "       ‚îú‚îÄ Pod Discovery (application metrics)"
echo "       ‚îî‚îÄ Service Discovery (service endpoints)"
echo "  - Grafana: Visualization"
echo ""
echo "Metrics Collection Details:"
echo "  ‚Ä¢ Node-level: Unix exporter (node_exporter equivalent) + Kubelet cAdvisor"
echo "  ‚Ä¢ Cluster-level: Pod discovery (annotation-based)"
echo "  ‚Ä¢ All metrics sent to Prometheus via remote_write"
echo "  ‚Ä¢ Operator-free, config-based discovery (no ServiceMonitor/PodMonitor)"
echo "  ‚Ä¢ K3s optimized: Uses containerd-native kubelet metrics"
echo ""
